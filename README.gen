Auto created from READMEs located in ZRT project

---------------------------------------------------------------
ZRT - ZeroVM runtime library
---------------------------------------------------------------
Editable README here: ./lib/README

1. ZRT is a part of libc, implementing syscalls and has own
filesystem. Direct syscalls from user code to host system are not
allowed, and such unsecure applications should not be validated by
ZeroVM, see ZeroVM docs.
2. Currently ZRT own 2 filesystems:
2.1 Rewritable filesystem, available in user space as mounted on "/"
root. ZRT creates it at runtime in memory, and injects it by real
directories/files if nvram feature used during startup before main()
entry point.  All changes made in this filesystem will lost at nexe
exit.
2.2 Channels mount filesystem - it is wrapper around zerovm channels,
it's mounted into "/dev" folder. It has static structure based on
channels aliases list, and interpret channels as devices. All channels
aliases listed in zerovm manifest file should be started with /dev/
alias, that represents their belonging to /dev/ mount. Channels whose
aliases not started with /dev/ will not be accessible by channels
filesystem. Functions like stat, readdir are supported also;
2.2.1 Channels representation by stat function. Sequential access
channels are represented as character devices; Random access channels
are represented as block devices;
2.2.2 Channels I/O. For channels described in manifest - input/output
operations are available, and folowing allowed :
Channel type SGetSPut=0, sequential  get / sequential_put
RDONLY     Seek pos —     get
WRONLY     Seek pos —     get
RDWR       Seek pos —     get
Channel type RGetSPut=1, Random get / sequential put
RDONLY     Seek pos — set get 
WRONLY     Seek pos —     get
RDWR       Write pos unavailable; Read Seek pos - set get;
Channel type SGetRPut=2, Sequential get / Random put
RDONLY     Seek pos —     get
WRONLY     Seek pos — set get
RDWR       Read pos unavailable; Write Seek pos - set get;
Channel type RGetRPut=3, Random get / Random put
RDONLY     Seek pos — set get
WRONLY     Seek pos —     get
RDWR       Seek pos — set get
2.2.3 Debugging channel. ZRT has its own debugging channel associated
with alias name "/dev/debug". If this channel is defined then all
debugging ZRT information will go into the debug channel If debug
channel is not defined no debug info will available (apart from system
logs). Debugging level are regulated by VERBOSITY environment
variables and support 1,2,3 values.

--------------------------------------------
Networking Library
--------------------------------------------
Editable README here: ./lib/networking/README

1. Networking Library allows to setup network channels using channels
configuration from manifest file and it's based on channel alias names
accessible runtime via readir function; Using of networking library
allows to setup networking channels set just use dynamical channel
aliases from configuration, that way eliminates most of hardcoded
data;
2. Object files belongs to this library are resides on
libnetworking.a, so use: LDFLAGS+=-lnetworking
3. Examples of usage by samples: disort, reqrep, wordcount

--------------------------------------------
Map-Reduce Library
--------------------------------------------
Editable README here: ./lib/mapreduce/README

1.Data flow description for MapReduce 32/128.
Every Map node has input file, every Reduce node has output file;
Input file with data for every Map node reading by blocks of
configured size in bytes and default size is 1MB. Block size can be
overrided by environment variable MAP_CHUNK_SIZE. Read chunk of data
is passed into user defined Map function, that parses it and create
HASHes for input keys and return key and value buffers. Size of
key/value are configurable and can be vary up to 16bytes for current
implementation;
At the next stage map node is sorting data by key and applying for
sorted data user defined Combine function that reduces data. This step
can optionally skipped if Combine function not defined. At the next
stage MapReduce library for every map node creating histogram and send
it each to other map nodes; Based on resulted histogram data from Map
nodes are distributing to reducers; Reducers are receiving sorted data
from 'leave' map nodes and then doing merge for every portion of
received data and applying Combine function if it's defined. Some
words about 'leave' map nodes - they are staying leave for Reducer
node while it has not received MAP_EXCLUDE packet data.In case if
Reducer recevied MAP_EXCLUDE packet it break waiting loop and calling
Reduce() function that finilize result and exiting job;
2.Object files belongs to this library are resides in libmapreduce.a
and also MapReduce uses Networking Library to get cluster distributed
configuration; In order to link it use folowing:
LDFLAGS=-lmapreduce -lnetworking
3. Examples of usage: samples/wordcount

--------------------------------------------
PJD - Filesystem tests
--------------------------------------------
Editable README here: ./tests/pjd-fstest-20080816/README

Some changes was made in pjd-fstests engine to support zerovm
filesystem testing.  fstest every time invoked by prove just send
command line into background process fstest.nexe test via zerovm stdin
channel, result of every test are retrieved by fstest script and
returned into test suite.
every invocation of run_test.sh run all pointed tests in the same
background process, after completion of all tests background process
fstest.nexe is closing by kill_ftest.sh

Run all zrtfs tests:
    #./run_zrtfs_tests.sh
Run single test
    #./run_test "tests/mkdir/00.t"
Run whole tests directory
    #./run_test "tests/mkdir -r -v"    

That's all. Enjoy.

Currently supported operating systems: Linux.
Currently supported file system types: zrtfs.

Author: 
Pawel Jakub Dawidek <pjd@FreeBSD.org>

Linux port:
Jean-Pierre Andre <jean-pierre.andre@wanadoo.fr>
Szabolcs Szakacsits <szaka@ntfs-3g.org>

ZeroVM port:
Yaroslav Litvinov <yaroslav.litvinov@gmail.com>
--------------------------------------------
Libc test suite
--------------------------------------------
Editable README here: ./tests/glibc_test_suite/README

Run glibc tests for zerovm using only one thing:
sh run_tests.sh

Some tests are using filesystem and to get it worked is used tarball
archive ./singles/archive/glibc-fs.tar added to manifest file of every
test, to be injected into nexes filesystem in memory; The main
interesting part of that archive are full list of localisation files
that used by glibc when nexe tries to setlocale;

Sections of tests which don't want be run:
./todo: 
Contains tests that require dependencies that should be ported;
./singles/xfail 
Contains all tests that fail at zerovm and at nacl glibc tests
./singles/xexcluded
Tests that was excluded in nacl glibc testsgmp
math/atest-sincos require libgmp (absent headers) seems to be should be ported
stdio-common/scanf.c require libiop (absent libioP.h)
posix/bug-regex5.c (absent localeinfo.h)
posix/bug-regex20.c (absent regex_internal.h)

crypt/testmd5.c (absent md5.h)
crypt/sha512test.c (absent sha512.h)
crypt/sha256test.c (absent sha256.h)

iconvdata/bug-iconv3.c (dlopen should be supported to load shared iconv libraries)
libio/tst-swscanf.c (locale "C" only works )
--------------------------------------------
ZRT test suite
--------------------------------------------
Editable README here: ./tests/zrt_test_suite/functional/README

This directory contains some functional tests for ZRT library and
autotests that running at every zrt build.

file_stat/
  demonstrates stat() / fstat() for various channels. Also it uses assertions to test expecting stat data; 
  
bigfile/
  It creates 5GB file, in case of error it generates assertion.
  
---------------------------------------------------
Lua Unit tests, based on Test suite for Lua 5.2.1
source: www.lua.org
---------------------------------------------------
Editable README here: ./tests/lua_test_suite/README

1.Lua test suite short brief.
  This suite contains nexe executable and test lua files:
  a) executable is used to load all tests at startup and just run lua
  script passed into command line of nexe; nexe executable is based on
  source file zrt/lib/lua-5.2.1/src/not_for_lib/lua.c that is part of
  original lua package ported to zerovm;
  b) test lua files are used inside of filesystem contains lua files
  and /tmp directory, and all of this are injected as tar archive via
  /dev/tarimage channel;
2.Build and run from command line:
cd zrt/tests/lua_test/suite
make
run_tests.sh
  



--------------------------------------------
Samples
--------------------------------------------
Editable README here: ./samples/README

Folowing samples demonstrates of usage and building for ZRT:
1.samples/tarimage - Nvram usage demo. User specifying tarball archive
and using nvram mechanism image contents are injecting into user file
system and listing it by readdir;
2.samples/zshell - run lua scripts and sql queries via single zerovm
executable and various manifest files. also available python
scripting;
3.samples/wordcount - distributed application that uses mapreduce and
networking libraries. It calculates count of words in provided input
files; One file per one map node using by design; 3.samples/disort -
distributed sort, 3 kind of applications source,destination, manager
that interacts as nodes, and implements networking communication;
Local sort uses high performance bitonic sort for sse41 instructions
set, or qsort from c std library for another cpu instructions
set. Currently network organized by 10 source, 10 destination and one
manager nodes. Count of nodes and amount of sorting data are possible
to config. Source data generates by generator application, and should
be run separatedly from sorting job;
4.samples/reqrep - this example uses network facility, it's emulate
simple network, where two nodes is communicating one to one. Can work
in two modes. mode 1 - Single node sending N MB and recevies the same
from another node. mode 2- one node sends and another node receives
data in cycle;
5.samples/hello - simple example how to write a program for zerovm.
6.samples/sort_paging - high performance sort contains 3 programs
which allows to create random data, sort it and test sort order. from
this example you can see how to use intrinsics under zerovm -
absolutely same as usual. also this is another example of pagination
mechanism usage. Doing check for sse41.
7.samples/time - via zerovm api user program can get date/time
specified in environment variable"TimeStamp". If there is no such env
var then user program will get -1 (1969-12-31 time = 23:59:59 UTC)

--------------------------------------------
Zshell
--------------------------------------------
Editable README here: ./samples/zshell/README

run LUA, SQLITE, PYTHON scripts from single application, first line of
script must contains char #lua / #sqlite / #python identifier.
Also required:
1. ZPYTHON_ROOT environment variable pointing to root folder of zpython;
2. compiled zpython;

--------------------------------------------
Distributed Sort
--------------------------------------------
Editable README here: ./samples/disort/README

1. Currently by default you can build and run sorting with 10 source and
10 destination nodes; You can run Distributed sort folowing this, in
terminal >
cd $(ZRT_ROOT)/samples/disort
make
./disort.sh

2. Nodes type of disort cluster
2.1main_node - Client of distributed sort that acts as single manager
node, and one instance is required;
2.2src_node - Client of distributed sort that acts as source node, Number
of src nodes can be configured
by SRC_NODES_COUNT from defines.h, at least 3 nodes should be set;
2.3dst_nodes - Same as src_node, but act as destination node with
predefined instances number;

3. Brief info of project contents
 ./disort.sh distributed sort executer
 [disort/manager] Files related to manager node
 [disort/manager/main_man.c] entry point to manager node
 [disort/histanlz.c] it contains functions related to histograms
 analizing
 [disort/comm_man.c] file IO related code  
 
 [disort/source] Source node code files
 [disort/source/main_src.c] entry point to source node
 [disort/source/comm_src.c] file IO related code
 
 [disort/dest] Destionation node code files
 [disort/dest/main_dst.c] entry point to dest node
 [disort/dest/comm_dst.c] file IO related code
 
 [manifest] manifest files of given sample
 [manifest/sortsrc.manifest.template]
 [manifest/sortdst.manifest.template] is template and used by genmanifest.sh script to generate manifests for nodes
 [manifest/sortman.manifest] Manager node manifest
 
4. Logging & Diagramm
Every node doing logging, logs are located in log/ dir.
Simplified networking data flow is in here:
https://github.com/YaroslavLitvinov/Distributed-Sort/blob/master/zeromq-diagram.pdf



--------------------------------------------
Samples
--------------------------------------------
Editable README here: ./samples/wordcount/README

Wordcount example for mapreduce library.  Every map node gets source
file as an input via stdin.  Every reduce node gets mapped key/values
from map nodes, reduces it and outputs to stdout.  Both types of nodes
are logging into stderr.
The mapreduce library is configured in runtime to use key size=4byte
and value size=4byte.  Value can be set up to 128 bytes then
configuring mapreduce library.  User Map function parses every chunk
of input data provided by MapReduce library as words delimeted by
space and '\0' (null terminated) characters and adds them to the
map(keys buffer, values buffer) as a pair.  The result is a count for
each hash key.

use folowing to run:
./mr_start.sh

